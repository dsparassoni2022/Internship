{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6590c971",
   "metadata": {},
   "source": [
    "# Web Scraping BeautifulSoup Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f682041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c96cbd",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da33af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_headers(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all header tags\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    # Extract header text and store in a list\n",
    "    header_text = [header.get_text() for header in headers]\n",
    "    \n",
    "    # Create a DataFrame from the header text list\n",
    "    df = pd.DataFrame(header_text, columns=['Header'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf79f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "df = scrape_headers(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1a480",
   "metadata": {},
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67f3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_former_presidents(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the former presidents\n",
    "    table = soup.find('table', class_='tablepress')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    names = []\n",
    "    terms = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        # Extract president's name\n",
    "        name = row.find('td', class_='column-1').text.strip()\n",
    "        names.append(name)\n",
    "        \n",
    "        # Extract president's term of office\n",
    "        term = row.find('td', class_='column-2').text.strip()\n",
    "        terms.append(term)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Name': names,\n",
    "        'Term of Office': terms\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "df = scrape_former_presidents(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d501cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a22eef4",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2e799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_mens_odi_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 teams\n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='u-hide-phablet').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract matches, points, and rating\n",
    "        cells = row.find_all('td')\n",
    "        match = cells[1].text.strip()\n",
    "        point = cells[2].text.strip()\n",
    "        rating = cells[3].text.strip()\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d4f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team           Matches Points Rating\n",
      "0     Australia    Australia\\nAUS     23  2,714\n",
      "1      Pakistan     Pakistan\\nPAK     20  2,316\n",
      "2         India        India\\nIND     33  3,807\n",
      "3   New Zealand   New Zealand\\nNZ     27  2,806\n",
      "4       England      England\\nENG     24  2,426\n",
      "5  South Africa  South Africa\\nSA     19  1,910\n",
      "6    Bangladesh   Bangladesh\\nBAN     25  2,451\n",
      "7   Afghanistan  Afghanistan\\nAFG     10    878\n",
      "8     Sri Lanka     Sri Lanka\\nSL     21  1,682\n",
      "9   West Indies   West Indies\\nWI     25  1,797\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "df = scrape_mens_odi_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe13b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "482bdb35",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e205de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_mens_odi_batsmen_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 players\n",
    "        # Extract player name\n",
    "        player = row.find('td', class_='table-body__cell name').text.strip()\n",
    "        players.append(player)\n",
    "        \n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='table-body__logo-text').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = row.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "df = scrape_mens_odi_batsmen_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15af0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00c2e4ac",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_mens_odi_bowlers_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 players\n",
    "        # Extract player name\n",
    "        player = row.find('td', class_='table-body__cell name').text.strip()\n",
    "        players.append(player)\n",
    "        \n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='table-body__logo-text').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = row.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99879e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "df = scrape_mens_odi_bowlers_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79874d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6cbb23",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bf2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 teams\n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='u-hide-phablet').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract matches, points, and rating\n",
    "        cells = row.find_all('td')\n",
    "        match = cells[1].text.strip()\n",
    "        point = cells[2].text.strip()\n",
    "        rating = cells[3].text.strip()\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Team': teams,\n",
    "        'Matches': matches,\n",
    "        'Points': points,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce7eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team           Matches Points Rating\n",
      "0     Australia    Australia\\nAUS     21  3,603\n",
      "1       England      England\\nENG     28  3,342\n",
      "2  South Africa  South Africa\\nSA     26  3,098\n",
      "3         India        India\\nIND     27  2,820\n",
      "4   New Zealand   New Zealand\\nNZ     25  2,553\n",
      "5   West Indies   West Indies\\nWI     27  2,535\n",
      "6      Thailand     Thailand\\nTHA     11    821\n",
      "7    Bangladesh   Bangladesh\\nBAN     14    977\n",
      "8      Pakistan     Pakistan\\nPAK     27  1,678\n",
      "9     Sri Lanka     Sri Lanka\\nSL      9    479\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "df = scrape_womens_odi_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b93d8",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f96f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_batting_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 players\n",
    "        # Extract player name\n",
    "        player = row.find('td', class_='table-body__cell name').text.strip()\n",
    "        players.append(player)\n",
    "        \n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='table-body__logo-text').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = row.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "df = scrape_womens_odi_batting_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceef38c",
   "metadata": {},
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0290f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_allrounder_rankings(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the rankings\n",
    "    table = soup.find('table', class_='table')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Extract the details from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]:  # Extract only the top 10 players\n",
    "        # Extract player name\n",
    "        player = row.find('td', class_='table-body__cell name').text.strip()\n",
    "        players.append(player)\n",
    "        \n",
    "        # Extract team name\n",
    "        team = row.find('span', class_='table-body__logo-text').text.strip()\n",
    "        teams.append(team)\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = row.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Player': players,\n",
    "        'Team': teams,\n",
    "        'Rating': ratings\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4018985",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "df = scrape_womens_odi_allrounder_rankings(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bd633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b921bc3",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e881b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_news_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the section containing the news articles\n",
    "    section = soup.find('div', class_='Latest-news-list-container')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "    \n",
    "    # Extract the details from each news article\n",
    "    articles = section.find_all('div', class_='Card-title')\n",
    "    for article in articles:\n",
    "        # Extract headline\n",
    "        headline = article.find('a').text.strip()\n",
    "        headlines.append(headline)\n",
    "        \n",
    "        # Extract time\n",
    "        time = article.find('span', class_='Card-time').text.strip()\n",
    "        times.append(time)\n",
    "        \n",
    "        # Extract news link\n",
    "        news_link = article.find('a')['href']\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Headline': headlines,\n",
    "        'Time': times,\n",
    "        'News Link': news_links\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96330fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "df = scrape_news_details(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24134e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef8d3964",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa60389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the section containing the most downloaded articles\n",
    "    section = soup.find('section', id='body')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    titles = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "    paper_urls = []\n",
    "    \n",
    "    # Extract the details from each article card\n",
    "    article_cards = section.find_all('li', class_='result-list-item')\n",
    "    for card in article_cards:\n",
    "        # Extract paper title\n",
    "        title = card.find('a', class_='result-list-title-link').text.strip()\n",
    "        titles.append(title)\n",
    "        \n",
    "        # Extract authors\n",
    "        author_tags = card.find_all('span', class_='author-list')\n",
    "        author_list = [author.text.strip() for author in author_tags]\n",
    "        authors.append(', '.join(author_list))\n",
    "        \n",
    "        # Extract published date\n",
    "        date = card.find('span', class_='published-online').text.strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Extract paper URL\n",
    "        paper_url = card.find('a', class_='result-list-title-link')['href']\n",
    "        paper_urls.append(paper_url)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Paper Title': titles,\n",
    "        'Authors': authors,\n",
    "        'Published Date': dates,\n",
    "        'Paper URL': paper_urls\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03821af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "df = scrape_most_downloaded_articles(url)\n",
    "print(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df968bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79ccb35",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac40567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_restaurant_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the restaurant cards\n",
    "    restaurant_cards = soup.find_all('div', class_='restnt-card')\n",
    "    \n",
    "    # Create lists to store the extracted data\n",
    "    names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "    \n",
    "    # Extract the details from each restaurant card\n",
    "    for card in restaurant_cards:\n",
    "        # Extract restaurant name\n",
    "        name = card.find('div', class_='restnt-name ellipsis').text.strip()\n",
    "        names.append(name)\n",
    "        \n",
    "        # Extract cuisine\n",
    "        cuisine = card.find('div', class_='restnt-cuisine').text.strip()\n",
    "        cuisines.append(cuisine)\n",
    "        \n",
    "        # Extract location\n",
    "        location = card.find('div', class_='restnt-loc ellipsis').text.strip()\n",
    "        locations.append(location)\n",
    "        \n",
    "        # Extract ratings\n",
    "        rating = card.find('div', class_='restnt-rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        # Extract image URL\n",
    "        image_url = card.find('div', class_='restnt-thumbnail').find('img')['data-src']\n",
    "        image_urls.append(image_url)\n",
    "    \n",
    "    # Create a dataframe using the extracted data\n",
    "    data = {\n",
    "        'Restaurant Name': names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "df = scrape_restaurant_details(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833905c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5234f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883119fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
